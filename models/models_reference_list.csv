Type,Features,General Category ,General Applications,Name of the Model,Citation,Website
Audio,Speech Separation,Speech,Speech Separation,Separate And Diffuse: Using a Pretrained Diffusion Model for Improving Source Separation,"Lutati, S., Nachmani, E., & Wolf, L. (2023). Separate and diffuse: Using a pretrained diffusion model for improving source separation. arXiv preprint arXiv:2301.10752.",NA
Audio,Speech Separation,Speech,Speech Separation,SepIt: Approaching a Single Channel Speech Separation Bound,"Lutati, S., Nachmani, E., & Wolf, L. (2022). Sepit: Approaching a single channel speech separation bound. arXiv preprint arXiv:2205.11801.",NA
Audio,Speech Separation,Speech,Speech Separation,SepFormer,"Subakan, C., Ravanelli, M., Cornell, S., Bronzi, M., & Zhong, J. (2021, June). Attention is all you need in speech separation. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 21-25). IEEE.",https://github.com/speechbrain/speechbrain
Audio,Speech Separation,Speech,Speech Separation,Dual-Path RNN,"Luo, Y., Chen, Z., & Yoshioka, T. (2020, May). Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 46-50). IEEE.",NA
Audio,Speech Separation,Speech,Speech Separation,Conv-TasNet: Surpassing Ideal Time–Frequency Magnitude Masking for Speech Separation,"Luo, Y., & Mesgarani, N. (2019). Conv-tasnet: Surpassing ideal time–frequency magnitude masking for speech separation. IEEE/ACM transactions on audio, speech, and language processing, 27(8), 1256-1266.",NA
Audio, Time-Accurate Speech Transcription,Speech,Speech-to-Text,WhisperX: Time-Accurate Speech Transcription of Long-Form Audio,"Bain, M., Huh, J., Han, T., & Zisserman, A. (2023). Whisperx: Time-accurate speech transcription of long-form audio. arXiv preprint arXiv:2303.00747.",https://github.com/m-bain/whisperX
Audio,-,Speech,Speech-to-Text,-,"Gállego, G. I., Tsiamas, I., Escolano, C., Fonollosa, J. A., & Costa-jussà, M. R. (2021). End-to-end speech translation with pre-trained models and adapters: Upc at iwslt 2021. arXiv preprint arXiv:2105.04512.",-
Audio,Speech-to-Text,Speech,Speech-to-Text,XLSR,"Conneau, A., Baevski, A., Collobert, R., Mohamed, A., & Auli, M. (2020). Unsupervised cross-lingual representation learning for speech recognition. arXiv preprint arXiv:2006.13979.",https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec
Audio,Speech-to-Text,Speech,Speech-to-Text,Speech-to-Text (S2T) Modeling,"Wang, C., Tang, Y., Ma, X., Wu, A., Popuri, S., Okhonko, D., & Pino, J. (2020). Fairseq S2T: Fast speech-to-text modeling with fairseq. arXiv preprint arXiv:2010.05171.",https: //github.com/pytorch/fairseq/tree/ master/examples/speech_to_text
Audio,Speech-to-Text,Speech,Speech-to-Text,SpecAugment,"Park, D. S., Chan, W., Zhang, Y., Chiu, C. C., Zoph, B., Cubuk, E. D., & Le, Q. V. (2019). Specaugment: A simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779.",NA
Audio,Disentangled Attention Mechanism:,Text,Text Embedding,DEBERTA,"He, P., Liu, X., Gao, J., & Chen, W. (2020). Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654.",https://github.com/microsoft/DeBERTa
Audio,Enhanced Mask Decoder,Text,Text Embedding,DEBERTA,"He, P., Liu, X., Gao, J., & Chen, W. (2020). Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654.",https://github.com/microsoft/DeBERTa
Audio,contrastive learning framework,Text,Text Embedding,SimCSE: Simple Contrastive Learning of Sentence Embeddings,"Gao, T., Yao, X., & Chen, D. (2021). Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821.",https://github.com/princeton-nlp/SimCSE
Audio,Language representation,Text,Text Embedding,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,"Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019). Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.",https://github.com/google-research/ALBERT
Audio,compute dense vector representations for sentences=,Text,Text Embedding,Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,"Reimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.",https://github.com/UKPLab/sentence-transformers
Audio,compute dense vector representations for paragraphs,Text,Text Embedding,Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,"Reimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.",https://github.com/UKPLab/sentence-transformers
Visual (language),compute dense vector representations for images,Text,Text Embedding,Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,"Reimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.",https://github.com/UKPLab/sentence-transformers
Audio,text classification,Text,Text Embedding,Universal Sentence Encoder,"Cer, D., Yang, Y., Kong, S. Y., Hua, N., Limtiaco, N., John, R. S., ... & Kurzweil, R. (2018). Universal sentence encoder. arXiv preprint arXiv:1803.11175.",https://tfhub.dev/google/universal-sentence-encoder/1
Audio,semantic similarity,Text,Text Embedding,Universal Sentence Encoder,"Cer, D., Yang, Y., Kong, S. Y., Hua, N., Limtiaco, N., John, R. S., ... & Kurzweil, R. (2018). Universal sentence encoder. arXiv preprint arXiv:1803.11175.",https://tfhub.dev/google/universal-sentence-encoder/1
Audio,semantic cluster,Text,Text Embedding,Universal Sentence Encoder,"Cer, D., Yang, Y., Kong, S. Y., Hua, N., Limtiaco, N., John, R. S., ... & Kurzweil, R. (2018). Universal sentence encoder. arXiv preprint arXiv:1803.11175.",https://tfhub.dev/google/universal-sentence-encoder/1
Audio,Sentiment Analysis,Text,Sentiment Analysis,AnAlgorithm for Routing Vectors in Sequences,"Heinsen, F. A. (2022). An algorithm for routing vectors in sequences. arXiv preprint arXiv:2211.11754； Heinsen, F. A. (2019). An algorithm for routing capsules in all domains. arXiv preprint arXiv:1911.00792.",https://github.com/glassroom/heinsen_routing
Audio,Sentiment Analysis,Text,Sentiment Analysis,Self-Explaining Structures Improve NLP Models,"Sun, Z., Fan, C., Han, Q., Sun, X., Meng, Y., Wu, F., & Li, J. (2020). Self-explaining structures improve nlp models. arXiv preprint arXiv:2012.01786.",https://github.com/ShannonAI/Self_Explaining_Structures_Improve_NLP_Models
Audio,complex characteristics of word use and how these uses vary across linguistic contexts ,Text,Sentiment Analysis,Deep contextualized word representations,"Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. arXiv. arXiv preprint arXiv:1802.05365.",https://paperswithcode.com/paper/deep-contextualized-word-representations
Audio,Speech emotion/emotional speech classification,Speech,Emotion Speech,Speech Emotion Recognition,Saravanaraj (2019). Emotion Recognition Using Speech.,https://github.com/x4nth055/emotion-recognition-using-speech
Audio,Audio volume,-,-,OpenCV,-,-
Audio,Change in audio volume ,-,-,OpenCV,-,-
Audio,Average audio pitch,-,-,OpenCV,-,-
Audio,Change in audio pitch,-,-,OpenCV,-,-
Audio,"Spectral Features, Pitch, Rhythm",Speech features,,Librosa,"McFee, B., et al. (2015). librosa: Audio and music signal analysis in python. SciPy",https://github.com/librosa/librosa
Audio,Speech feature extraction,Speech/Music,"processing and classification especially targeted at speech and music applications, e.g. automatic speech recognition, speaker identification, emotion recognition, or beat tracking and chord detection.",openSMILE,"Eyben, F., et al. (2010). openSMILE The Munich Versatile and Fast Open Source Audio Feature Extractor. ACM MM",GitHub - audeering/opensmile: The Munich Open-Source Large-Scale Multimedia Feature Extractor
Audio,High-quality time-stretching of WAV/MP3 files without changing their pitch,Time stretching,Time stretching,AudioStretchy,-; Note: you can directly refer the github website,"GitHub - twardoch/audiostretchy: AudioStretchy is a Python wrapper around the `audio-stretch` C library, which performs fast, high-quality time-stretching of WAV/MP3 files without changing their pitch. Works well for speech, can time-stretch silence separately."
Audio,Time-stretch silence separately,Time stretching,Time stretching,AudioStretchy,-; Note: you can directly refer the github website,"GitHub - twardoch/audiostretchy: AudioStretchy is a Python wrapper around the `audio-stretch` C library, which performs fast, high-quality time-stretching of WAV/MP3 files without changing their pitch. Works well for speech, can time-stretch silence separately."
Image [single in-the-wild image],"There is no obvious github/pre-trained model resources, might be refer the original article. ",Facial Expression,Emotion Labelling,Pyramid with super-resolution (PSR) network,"Vo TH, Lee GS, Yang HJ, Kim SH. Pyramid with super resolution for in-the-wild facial expression recognition. IEEE Access. 2020 Jul 17;8:131988-2001.",NA
Multimoal (Audio + Visual),Emotion recognition during the social interactions,Coversations and emotion detection,Coversations and emotion detection,MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversation,"Poria, S., et al. (2019). MELD: A multimodal multi party dataset for emotion recognition in conversations. ACL.",GitHub - declare-lab/MELD: MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversation
Multimoal (Audio + Visual),Audio-Visual Analysis,audio-visual speech,Audio-Visual Analysis,AV HuBERT,"Shi, B., et al. (2022). Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction. arXiv:2201.02184.",https://github.com/facebookresearch/av_hubert
Video,Emotional expression indices,Facial Expression,Emotion Labelling,DAN: Distract Your Attention: Multi-head Cross Attention Network for Facial Expression Recognition,"Wen Z, Lin W, Wang T, Xu G. Distract your attention: Multi-head cross attention network for facial expression recognition. Biomimetics. 2023 May 11;8(2):199.",GitHub - yaoing/DAN: Official implementation of DAN
Video,"There is no obvious github/pre-trained model resources, might be refer the original article. ",Facial Expression,Emotion Labelling,Region Attention Network (RAN),"Wang, K., Peng, X., Yang, J., Meng, D., & Qiao, Y. (2020). Region attention networks for pose and occlusion robust facial expression recognition. IEEE Transactions on Image Processing, 29, 4057-4069.",NA
Video,Facial action + AU relation graph,Facial Expression,Action Unit Annotation,Learning Multi-dimensional Edge Feature-based AU Relation Graph for Facial Action Unit Recognition,"Luo, C., Song, S., Xie, W., Shen, L., & Gunes, H. (2022). Learning Multi-dimensional Edge Feature based AU Relation Graph for Facial Action Unit Recognition. Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, 1239–1246. https://doi.org/10.24963/ijcai.2022/173",https://github.com/CVI-SZU/ME-GraphAU
Video,Predict body-part-guided attention masks. Or the hidden posture,Body Pose,Pose Estimation,PARE: Part attention regressor for 3D human body estimation,"Kocabas, M., Huang, C. H. P., Hilliges, O., & Black, M. J. (2021). PARE: Part attention regressor for 3D human body estimation. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 11127-11137).",https://pare.is.tue.mpg.de/
Video,Pose estimation,Body Pose,Pose Estimation,ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation,"Xu, Y., Zhang, J., Zhang, Q., & Tao, D. (2022). Vitpose: Simple vision transformer baselines for human pose estimation. Advances in neural information processing systems, 35, 38571-38584.",https://github.com/ViTAE-Transformer/ViTPose
Video,Estimating keypoint heatmaps and segmentation masks,Body Pose,Pose Estimation,Polarized Self-Attention (PSA),"Liu, H., Liu, F., Fan, X., & Huang, D. (2022). Polarized self-attention: Towards high-quality pixel-wise mapping. Neurocomputing, 506, 158-167.",https://github.com/DeLightCMU/PSA
Video,Keypoint localization,Body Pose,Pose Estimation,Residual Steps Network (RSN),"Cai, Y., Wang, Z., Luo, Z., Yin, B., Du, A., Wang, H., ... & Sun, J. (2020, August). Learning delicate local representations for multi-person pose estimation. In European conference on computer vision (pp. 455-472). Cham: Springer International Publishing.",https://github.com/caiyuanhao1998/RSN/
Video,Bidirectional information transfer between action/frame features,Social Actions,Temporal Action Segmentation,FACT: Frame-Action Cross-Attention Temporal Modeling for Efficient Action Segmentation,"Lu, Z., & Elhamifar, E. (2024). Fact: Frame-action cross-attention temporal modeling for efficient action segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 18175-18185).",https://github.com/ZijiaLewisLu/CVPR2024-FACT
Video (real time),"(1) Real time video (emotional expression analytic/naming which type of emotion such as sad, happiness, etc);",Facial Expression,Emotional Labelling/Naming,"Frame-level Prediction of Facial Expressions, Valence, Arousal and Action Units for Mobile Devices","Savchenko, A. V. (2022). Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices. arXiv preprint arXiv:2203.13436.",EmotiEffLib/models/affectnet_emotions at main · av-savchenko/EmotiEffLib · GitHub
Video (real time), (2) The prediction of valence and arousal and detection of action unit points,Facial Expression,Action Unit Annotation/The prediction of valence and arousal and detection of action unit points,"Frame-level Prediction of Facial Expressions, Valence, Arousal and Action Units for Mobile Devices","Savchenko, A. V. (2022). Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices. arXiv preprint arXiv:2203.13436.",EmotiEffLib/models/affectnet_emotions at main · av-savchenko/EmotiEffLib · GitHub
Video/Image,Emotional indices,Facial Expression,Emotion Labelling,Py-Feat,"Cheong, J. H., Jolly, E., Xie, T., Byrne, S., Kenney, M., & Chang, L. J. (2023). Py-feat: Python facial expression analysis toolbox. Affective Science, 4(4), 781-796.",Py-Feat: Python Facial Expression Analysis Toolbox — Py-Feat
Video/Image,extract the emotional indices; utilizing by different levels of features (such as low/high and multi-levels). ,Facial Expression,Emotion Labelling,ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning,"Wasi, A. T., Šerbetar, K., Islam, R., Rafi, T. H., & Chae, D. K. (2023). ARBEx: Attentive feature extraction with reliability balancing for robust facial expression learning. arXiv preprint arXiv:2305.01486.",https://github.com/takihasan/ARBEx
Video/Image,extract the distinctive features of faces during the experimental task. ,Facial Expression,Emotion Labelling,Attentive Pooling (AP) modules with Vision Transformer (APViT),"Xue, F., Wang, Q., Tan, Z., Ma, Z., & Guo, G. (2022). Vision transformer with attentive pooling for robust facial expression recognition. IEEE Transactions on Affective Computing, 14(4), 3244-3256.","1. The pre-trained weight is downloaded from https://github.com/
ZhaoJ9014/face.evoLVe.PyTorch#Model-Zoo.
2. The pre-trained weight is downloaded from https://github.com/
rwightman/pytorch-image-models/."
Video/Image,actional annotation,Facial Expression,Action Unit Annotation,Py-Feat,"Cheong, J. H., Jolly, E., Xie, T., Byrne, S., Kenney, M., & Chang, L. J. (2023). Py-feat: Python facial expression analysis toolbox. Affective Science, 4(4), 781-796.",Py-Feat: Python Facial Expression Analysis Toolbox — Py-Feat
Video/Image,Using continuous manifold the anatomical facial movements defining a human expression from single image,Facial Expression,Action Unit Annotation,GANimation,"Pumarola, A., Agudo, A., Martinez, A. M., Sanfeliu, A., & Moreno-Noguer, F. (2018). Ganimation: Anatomically-aware facial animation from a single image. In Proceedings of the European conference on computer vision (ECCV) (pp. 818-833).",GitHub - albertpumarola/GANimation: GANimation: Anatomically-aware Facial Animation from a Single Image (ECCV'18 Oral) [PyTorch]
Video/Image,pose estimation and tracking,Body Pose,Pose Estimation,Google MediaPipe,"See the website on the right, the guide",https://ai.google.dev/edge/mediapipe/solutions/guide
Video/Image,pose estimation/precise position,Body Pose,Pose Estimation,Deep High-Resolution Representation Learning for Human Pose Estimation,"Sun, K., Xiao, B., Liu, D., & Wang, J. (2019). Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 5693-5703).", https://github.com/leoxiaobin/deep-high-resolution-net.pytorch
Video/Image,pose estimation and tracking,Body Pose,Pose Estimation,Simple Baselines for Human Pose Estimation and Tracking,"Xiao, B., Wu, H., & Wei, Y. (2018). Simple baselines for human pose estimation and tracking. In Proceedings of the European conference on computer vision (ECCV) (pp. 466-481).",https://github.com/Microsoft/human-pose-estimation.pytorch
Video/Image ,pose estimation and tracking,Body Pose,Pose Estimation,Open Pose,"See the website on the right, the guide",https://github.com/CMU-Perceptual-Computing-Lab/openpose
Video/Image/Visual,Good features to track and calculate the displacements,Opyflow is a basic image velocimetry tool to simplify your video or frame sequences processing.,Optical Flow,Opyflow,,https://github.com/groussea/opyflow
Video/Visual,"Depth in dynamic scene, [such as the interactions between individuals and position in the video and their movement patterns in the video]",movement indices,depth in the scene/videos,Learning Monocular Depth in Dynamic Scenes via Instance-Aware Projection Consistency,"Lee, S., Im, S., Lin, S., & Kweon, I. S. (2021, May). Learning monocular depth in dynamic scenes via instance-aware projection consistency. In Proceedings of the AAAI conference on artificial intelligence (Vol. 35, No. 3, pp. 1863-1872).",https://github.com/x4nth055/emotion-recognition-using-speech
Video/Visual,movement  and the estimation of motion,motion estimation model,motion.movement estimation,Optical Flow,The tutorial can be checked the general website here: https://nanonets.com/blog/optical-flow/,https://github.com/chuanenlin/optical-flow
Video/Visual,Optical flow fields,Optical Flow,Visual crowded analysis,Optical Flow Dataset and Benchmark for Visual Crowd Analysis,"Schröder, G., Senst, T., Bochinski, E., & Sikora, T. (2018, November). Optical flow dataset and benchmark for visual crowd analysis. In 2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS) (pp. 1-6). IEEE.",https://github.com/tsenst/CrowdFlow
Video/Visual,Person trajectories (up to 1451),Optical Flow,Visual crowded analysis,Optical Flow Dataset and Benchmark for Visual Crowd Analysis,"Schröder, G., Senst, T., Bochinski, E., & Sikora, T. (2018, November). Optical flow dataset and benchmark for visual crowd analysis. In 2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS) (pp. 1-6). IEEE.",https://github.com/tsenst/CrowdFlow
Video/Visual,Dense pixel trajectories,Optical Flow,Visual crowded analysis,Optical Flow Dataset and Benchmark for Visual Crowd Analysis,"Schröder, G., Senst, T., Bochinski, E., & Sikora, T. (2018, November). Optical flow dataset and benchmark for visual crowd analysis. In 2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS) (pp. 1-6). IEEE.",https://github.com/tsenst/CrowdFlow
Video/Visual,Dense Motion Estimation,-,Motion estimation,Insta-DM,"Lee, S., Im, S., Lin, S., & Kweon, I. S. (2021, May). Learning monocular depth in dynamic scenes via instance-aware projection consistency. In Proceedings of the AAAI conference on artificial intelligence (Vol. 35, No. 3, pp. 1863-1872).",GitHub - SeokjuLee/Insta-DM: Learning Monocular Depth in Dynamic Scenes via Instance-Aware Projection Consistency (AAAI 2021)
Video/Visual,Locate the objects and people,Video Analysis,Video Analysis,VideoFinder,No direct research article reference. ,"GitHub - win4r/VideoFinder-Llama3.2-vision-Ollama: VideoFinder is an advanced video analysis tool powered by multimodal AI, designed to help users easily locate and identify specific objects or people within video content. By combining the capabilities of Llama Vision model with a streamlined web interface, it enables real-time, frame-by-frame video analysis with natural language descriptions."
Video/Visual,Video frame extraction (capture the key frame of the videos),capture key frames from videos,capture key frames from videos,Video Frame Extractor,No direct research article reference. ,"GitHub - BSM0oo/intelligent-video-frame-extractor: An AI-powered tool that intelligently extracts and analyzes key frames from videos using computer vision and GPT-4 Vision. Perfect for educational content, presentations, and lectures."
Video/Visual ,Real-Time Intermediate Flow Estimation,Motion Estimation,Motion Estimation,RIFE (Real Time Intermediate Flow Estimation),"Huang, Z., Zhang, T., Heng, W., Shi, B., & Zhou, S. (2022, October). Real-time intermediate flow estimation for video frame interpolation. In European Conference on Computer Vision (pp. 624-642). Cham: Springer Nature Switzerland.",GitHub - hzwer/ECCV2022-RIFE: ECCV2022 - Real-Time Intermediate Flow Estimation for Video Frame Interpolation
Video/Visual ,Pose estimation,"Human motions, pose estimators",Motion/pose estimation,SmoothNet,"Zeng, A., Yang, L., Ju, X., Li, J., Wang, J., & Xu, Q. (2022, October). Smoothnet: A plug-and-play network for refining human poses in videos. In European Conference on Computer Vision (pp. 625-642). Cham: Springer Nature Switzerland.","GitHub - cure-lab/SmoothNet: [ECCV 2022] Official implementation of the paper ""SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos"""
Video/Visual ,Autonomous Driving,motion forecasting model,motion forecasting model,LaneGCN: Learning Lane Graph Representations for Motion Forecasting,"Liang, M., Yang, B., Hu, R., Chen, Y., Liao, R., Feng, S., & Urtasun, R. (2020). Learning lane graph representations for motion forecasting. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16 (pp. 541-556). Springer International Publishing.",GitHub - uber-research/LaneGCN: [ECCV2020 Oral] Learning Lane Graph Representations for Motion Forecasting